<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2022/03/18/hello-world/</url>
    <content><![CDATA[<p>this is my frist program  </p>
<h2 id="Welcome-！"><a href="#Welcome-！" class="headerlink" title="Welcome ！"></a>Welcome ！</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	printf(&quot;Hello World!&quot;)</span><br><span class="line">	return 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


]]></content>
  </entry>
  <entry>
    <title>使用QT编写的代码编辑器</title>
    <url>/2022/03/18/codeedit/</url>
    <content><![CDATA[<p>使用QT编写的代码编辑器,采用MVC模式架构，数据与代码分离</p>
<span id="more"></span>

<h3 id="使用QT编写的代码编辑器"><a href="#使用QT编写的代码编辑器" class="headerlink" title="使用QT编写的代码编辑器"></a>使用QT编写的代码编辑器</h3><p>界面展示</p>
<p><img src="/2022/03/18/codeedit/main.png"></p>
<p>核心编辑框使用外部库QScilllina</p>
<p>采用MVC模式架构，数据与代码分离</p>
<p>TreeMenu实现不同Item映射不同菜单</p>
<p><img src="/2022/03/18/codeedit/menu.gif"></p>
]]></content>
  </entry>
  <entry>
    <title>pinn相关研究与研究进展</title>
    <url>/2022/03/18/pinn/</url>
    <content><![CDATA[<p>pinn相关研究与研究进展,相关论文总结与文献综述</p>
<span id="more"></span>

<h1 id="pinn求解非线性PDE"><a href="#pinn求解非线性PDE" class="headerlink" title="pinn求解非线性PDE"></a>pinn求解非线性PDE</h1><h2 id="引入相关库文件"><a href="#引入相关库文件" class="headerlink" title="引入相关库文件"></a>引入相关库文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="构造网络"><a href="#构造网络" class="headerlink" title="构造网络"></a>构造网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 继承torch的nn模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化构造函数</span></span><br><span class="line">    <span class="comment"># hidden_num:   隐藏层数量</span></span><br><span class="line">    <span class="comment"># vertices_num: 每层神经元个数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_num, vertices_num</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入层</span></span><br><span class="line">        self.input_layer = nn.Linear(<span class="number">2</span>, vertices_num)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 隐藏层</span></span><br><span class="line">        self.hidden_layers = nn.ModuleList([nn.Linear(vertices_num, vertices_num) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(hidden_num)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        self.output_layer = nn.Linear(vertices_num, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        o = self.act(self.input_layer(x))</span><br><span class="line">        <span class="keyword">for</span> i, li <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.hidden_layers):</span><br><span class="line">            o = self.act(li(o))</span><br><span class="line">        out = self.output_layer(o)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 激活函数:反正切</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">act</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * torch.sigmoid(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置加速器:Adam</span></span><br><span class="line">    <span class="comment"># lr: 误差控制</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_optimizer</span>(<span class="params">self, lr</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.optim.Adam(self.parameters(), lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="损失函数构造"><a href="#损失函数构造" class="headerlink" title="损失函数构造"></a>损失函数构造</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 求偏导函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradients</span>(<span class="params">u, x, order=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> order == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),</span><br><span class="line">                                   create_graph=<span class="literal">True</span>,</span><br><span class="line">                                   only_inputs=<span class="literal">True</span>, )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> gradients(gradients(u, x), x, order=order - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">interior</span>(<span class="params">size=<span class="number">2000</span></span>):</span><br><span class="line">    x = -<span class="number">1</span> + <span class="number">2</span> * torch.rand(size, <span class="number">1</span>)</span><br><span class="line">    t = torch.rand(size, <span class="number">1</span>)</span><br><span class="line">    cond = torch.zeros(size, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x.requires_grad_(<span class="literal">True</span>), t.requires_grad_(<span class="literal">True</span>), cond.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">boundary2</span>(<span class="params">size=<span class="number">2000</span></span>):</span><br><span class="line">    x = <span class="number">1</span> - <span class="number">2</span> * torch.ones(size, <span class="number">1</span>)</span><br><span class="line">    t = torch.rand(size, <span class="number">1</span>)</span><br><span class="line">    cond = torch.zeros(size, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x.requires_grad_(<span class="literal">True</span>), t.requires_grad_(<span class="literal">True</span>), cond.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">boundary1</span>(<span class="params">size=<span class="number">2000</span></span>):</span><br><span class="line">    x = torch.ones(size, <span class="number">1</span>)</span><br><span class="line">    t = torch.rand(size, <span class="number">1</span>)</span><br><span class="line">    cond = torch.zeros(size, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x.requires_grad_(<span class="literal">True</span>), t.requires_grad_(<span class="literal">True</span>), cond.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_value</span>(<span class="params">size=<span class="number">2000</span></span>):</span><br><span class="line">    x = torch.rand(size, <span class="number">1</span>)</span><br><span class="line">    t = torch.zeros(size, <span class="number">1</span>)</span><br><span class="line">    cond = -torch.sin(np.pi * x)</span><br><span class="line">    <span class="keyword">return</span> x.requires_grad_(<span class="literal">True</span>), t.requires_grad_(<span class="literal">True</span>), cond.requires_grad_(<span class="literal">True</span>).requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_interior</span>(<span class="params">net</span>):</span><br><span class="line">    x, t, cond = interior()</span><br><span class="line">    uxy = net(torch.cat([x, t], dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> loss(gradients(uxy, t, <span class="number">1</span>) + gradients(uxy, x, <span class="number">1</span>) * uxy - (<span class="number">0.01</span> / np.pi) * gradients(uxy, x, <span class="number">2</span>), cond)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_boundary2</span>(<span class="params">net</span>):</span><br><span class="line">    x, t, cond = boundary2()</span><br><span class="line">    uxy = net(torch.cat([x, t], dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> loss(uxy, cond)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_boundary1</span>(<span class="params">net</span>):</span><br><span class="line">    x, t, cond = boundary1()</span><br><span class="line">    uxy = net(torch.cat([x, t], dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> loss(uxy, cond)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_initial_value</span>(<span class="params">net</span>):</span><br><span class="line">    x, t, cond = initial_value()</span><br><span class="line">    uxy = net(torch.cat([x, t], dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> loss(uxy, cond)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Net(<span class="number">10</span>, <span class="number">24</span>)</span><br><span class="line"></span><br><span class="line">loss_data = []</span><br><span class="line"></span><br><span class="line">opt = torch.optim.Adam(params=net.parameters(), lr=<span class="number">1e-7</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    opt.zero_grad()</span><br><span class="line">    l = loss_interior(net) + loss_boundary1(net) + loss_initial_value(net) + loss_boundary2(net)</span><br><span class="line">    l.backward()</span><br><span class="line">    opt.step()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="built_in">print</span>(l.item())</span><br><span class="line">    loss_data.append([i, l.item()])</span><br><span class="line"></span><br><span class="line">torch.save(net, <span class="string">&#x27;./data/net.pkl&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>pinn</tag>
      </tags>
  </entry>
  <entry>
    <title>mainpage</title>
    <url>/2022/03/18/mainpage/</url>
    <content><![CDATA[<p>记录一些常见的latex公式，方便书写</p>
<span id="more"></span>

<h3 id="1-数学公式"><a href="#1-数学公式" class="headerlink" title="1.数学公式"></a>1.数学公式</h3><table>
<thead>
<tr>
<th>符号</th>
<th>公式</th>
<th></th>
<th>符号</th>
<th>公式</th>
</tr>
</thead>
<tbody><tr>
<td>\int</td>
<td>$\int$</td>
<td></td>
<td>a_1</td>
<td>$a_1$</td>
</tr>
<tr>
<td>\iint</td>
<td>$\iint$</td>
<td></td>
<td>a^2</td>
<td>$a^2$</td>
</tr>
<tr>
<td>\iiint</td>
<td>$\iiint$</td>
<td></td>
<td>\bar{x}</td>
<td>$\bar{x}$</td>
</tr>
<tr>
<td>\lim</td>
<td>$\lim_{n =1}$</td>
<td></td>
<td>\vec{x}</td>
<td>$\vec{x}$</td>
</tr>
<tr>
<td>\lim\limits_{n = 1}</td>
<td>$\lim\limits_{n=1}$</td>
<td></td>
<td>\dots</td>
<td>$x_1,x_2,\dots,x_n$</td>
</tr>
<tr>
<td>\sum</td>
<td>$\sum$</td>
<td></td>
<td>\cdots</td>
<td>$x_1,x_2,\cdots,x_n$</td>
</tr>
<tr>
<td>\prod</td>
<td>$\prod$</td>
<td></td>
<td>\ddots</td>
<td>$x_1,x_2,\ddots,x_n$</td>
</tr>
<tr>
<td>\sum\limits_{n = 1}^{\infty}</td>
<td>$\sum\limits_{n = 1}^{\infty}$</td>
<td></td>
<td>\vdots</td>
<td>$x_1,x_2,\vdots,x_n$</td>
</tr>
<tr>
<td>\dfrac{f(x)}{g(x)}</td>
<td>$\dfrac{f(x)}{g(x)}$</td>
<td></td>
<td>矩阵</td>
<td>$\begin{array}{12}x_1&amp;x_2&amp;\dots\x_3&amp;x_4&amp;\dots\\vdots&amp;\vdots&amp;\ddots\end{array}$</td>
</tr>
<tr>
<td>\tfrac{f(x)}{g(x)}(默认)</td>
<td>$\tfrac{f(x)}{g(x)}$</td>
<td></td>
<td>矩阵</td>
<td>$\begin{pmatrix} a &amp; b\ c &amp; d \ \end{pmatrix} \quad  \quad \begin{Bmatrix} a &amp; b \ c &amp; d\ \end{Bmatrix}\quad \begin{vmatrix} a &amp; b \ c &amp; d \ \end{vmatrix}\quad \begin{Vmatrix} a &amp; b\ c &amp; d \ \end{Vmatrix}$</td>
</tr>
<tr>
<td>\boxed{a}</td>
<td>$\boxed{a}$</td>
<td></td>
<td>\quad</td>
<td>$a\quad b$</td>
</tr>
<tr>
<td>\sqrt[n]{x}</td>
<td>$\sqrt[n]{x}$</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>split对齐</th>
<th align="right">$\begin{split}f(x)=&amp;ax^2+bx+c\&amp;dy^2+fy+e\end{split}$</th>
</tr>
</thead>
<tbody><tr>
<td>case</td>
<td align="right">$\begin{cases}f(x)=ax^2+bx+c\g(x)=dy^2+fy+e\end{cases}$</td>
</tr>
<tr>
<td>align对齐</td>
<td align="right">$\begin{align}f(x)=&amp;ax^2+bx+c\g(x)=&amp;dy^2+fy+e\end{align}$</td>
</tr>
<tr>
<td>array</td>
<td align="right">$\begin{array}{12}x_1&amp;x_2&amp;\dots\x_3&amp;x_4&amp;\dots\\vdots&amp;\vdots&amp;\ddots\end{array}$</td>
</tr>
</tbody></table>
<p><img src="/2022/03/18/mainpage/QQ%E6%88%AA%E5%9B%BE20200304195537.png"></p>
]]></content>
      <tags>
        <tag>main</tag>
      </tags>
  </entry>
</search>
